{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "recovered-sensitivity",
   "metadata": {},
   "source": [
    "# Analyse des Données Brutes HDFS et Séparation en Entraînement/Test\n",
    "\n",
    "Ce notebook examine les données brutes du fichier de logs HDFS (`HDFS.log`) et les divise en deux ensembles : un ensemble d'entraînement et un ensemble de test. L'objectif est de créer une base de données pour entraîner un modèle de détection d'anomalies dans les logs, en s'appuyant sur l’outil de parsing `Drain`. Ce dernier génère des \"templates\" (modèles) de messages pour les logs structurés, ce qui simplifie l'identification de motifs d'anomalies.\n",
    "\n",
    "Les données HDFS utilisées proviennent de la collection [Loghub](https://github.com/logpai/loghub), une grande base de données de logs systèmes. L'ensemble de données utilisé contient également des annotations pour chaque bloc indiquant s'il est normal ou anormal, ce qui facilite la validation du modèle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5309960",
   "metadata": {},
   "source": [
    "## Importation des Bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "norwegian-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importation des bibliothèques essentielles pour l'analyse et la manipulation des données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-australia",
   "metadata": {},
   "source": [
    "## Exploration Préliminaire des Données\n",
    "\n",
    "Dans cette section, nous allons examiner les données brutes dans le fichier `HDFS.log`. Cela inclut le comptage des lignes pour comprendre la taille des données et l'observation des premières lignes pour obtenir un aperçu de la structure des logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "parallel-democrat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier HDFS.log contient un total de 11175629 lignes\n"
     ]
    }
   ],
   "source": [
    "# Compter le nombre de lignes dans le fichier HDFS_2k.log\n",
    "with open('DataSet_HDFS/HDFS.log', \"r\") as file:\n",
    "    totaln = sum(1 for line in file)\n",
    "print(f'Le fichier HDFS.log contient un total de {totaln} lignes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fancy-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher un aperçu rapide des 200 premières lignes du fichier\n",
    "with open('DataSet_HDFS/HDFS.log', \"r\") as file:\n",
    "    data = list(islice(file, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "surprised-guard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log Entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>081109 203518 143 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>081109 203518 35 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>081109 203519 143 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>081109 203519 145 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>081109 203519 145 INFO dfs.DataNode$PacketResp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Log Entries\n",
       "0  081109 203518 143 INFO dfs.DataNode$DataXceive...\n",
       "1  081109 203518 35 INFO dfs.FSNamesystem: BLOCK*...\n",
       "2  081109 203519 143 INFO dfs.DataNode$DataXceive...\n",
       "3  081109 203519 145 INFO dfs.DataNode$DataXceive...\n",
       "4  081109 203519 145 INFO dfs.DataNode$PacketResp..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les premières lignes sous forme de DataFrame\n",
    "df = pd.DataFrame(data, columns=['Log Entries'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb2092e",
   "metadata": {},
   "source": [
    "islice(file, 200) : Utilise islice pour lire seulement les 200 premières lignes du fichier. islice (de la bibliothèque itertools) est une fonction pratique pour extraire une plage d'éléments sans boucle explicite.\n",
    "Conversion en DataFrame : On peut directement transformer les lignes en DataFrame avec une seule colonne appelée \"Log Entries\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "imposed-ministry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['081109 203523 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.100:50010 is added to blk_-3544583377289625738 size 11971\\n'],\n",
       "       ['081109 203523 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.197.226:50010 is added to blk_-3544583377289625738 size 11971\\n'],\n",
       "       ['081109 203524 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-9073992586687739851 src: /10.250.14.196:39576 dest: /10.250.14.196:50010\\n'],\n",
       "       ['081109 203524 144 INFO dfs.DataNode$DataXceiver: Receiving block blk_-9073992586687739851 src: /10.250.7.244:39735 dest: /10.250.7.244:50010\\n'],\n",
       "       ['081109 203524 145 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-9073992586687739851 terminating\\n'],\n",
       "       ['081109 203524 145 INFO dfs.DataNode$PacketResponder: Received block blk_-9073992586687739851 of size 11977 from /10.250.19.102\\n'],\n",
       "       ['081109 203524 146 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-9073992586687739851 terminating\\n'],\n",
       "       ['081109 203524 146 INFO dfs.DataNode$PacketResponder: Received block blk_-9073992586687739851 of size 11977 from /10.250.14.196\\n'],\n",
       "       ['081109 203524 147 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-9073992586687739851 terminating\\n'],\n",
       "       ['081109 203524 147 INFO dfs.DataNode$PacketResponder: Received block blk_-9073992586687739851 of size 11977 from /10.250.7.244\\n'],\n",
       "       ['081109 203524 19 INFO dfs.FSNamesystem: BLOCK* ask 10.251.215.16:50010 to replicate blk_-1608999687919862906 to datanode(s) 10.251.74.79:50010 10.251.107.19:50010\\n'],\n",
       "       ['081109 203524 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.89.155:50010 is added to blk_-9073992586687739851 size 11977\\n'],\n",
       "       ['081109 203524 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.244:50010 is added to blk_-9073992586687739851 size 11977\\n'],\n",
       "       ['081109 203524 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.14.196:50010 is added to blk_-9073992586687739851 size 11977\\n'],\n",
       "       ['081109 203525 150 INFO dfs.DataNode$DataXceiver: 10.251.215.16:50010 Served block blk_7503483334202473044 to /10.250.19.102\\n'],\n",
       "       ['081109 203526 146 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.251.215.16:51177 dest: /10.251.215.16:50010 of size 91178\\n'],\n",
       "       ['081109 203526 146 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.251.74.79:60333 dest: /10.251.74.79:50010 of size 91178\\n'],\n",
       "       ['081109 203526 146 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.251.215.16:51177 dest: /10.251.215.16:50010\\n'],\n",
       "       ['081109 203526 146 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.251.74.79:60333 dest: /10.251.74.79:50010\\n'],\n",
       "       ['081109 203526 149 INFO dfs.DataNode$DataXceiver: 10.251.39.179:50010 Served block blk_-3544583377289625738 to /10.250.18.114\\n'],\n",
       "       ['081109 203526 150 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.251.199.225\\n'],\n",
       "       ['081109 203526 151 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.251.30.85\\n'],\n",
       "       ['081109 203526 152 INFO dfs.DataNode$DataTransfer: 10.251.215.16:50010:Transmitted block blk_-1608999687919862906 to /10.251.74.79:50010\\n'],\n",
       "       ['081109 203526 19 INFO dfs.DataNode: 10.251.215.16:50010 Starting thread to transfer block blk_-1608999687919862906 to 10.251.74.79:50010, 10.251.107.19:50010\\n'],\n",
       "       ['081109 203526 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.74.79:50010 is added to blk_-1608999687919862906 size 91178\\n'],\n",
       "       ['081109 203526 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.19:50010 is added to blk_-1608999687919862906 size 91178\\n'],\n",
       "       ['081109 203527 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.251.31.5:53020 dest: /10.251.31.5:50010\\n'],\n",
       "       ['081109 203527 147 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.251.107.19:56341 dest: /10.251.107.19:50010 of size 91178\\n'],\n",
       "       ['081109 203527 147 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.251.107.19:56341 dest: /10.251.107.19:50010\\n'],\n",
       "       ['081109 203527 148 INFO dfs.DataNode$DataTransfer: 10.251.107.19:50010:Transmitted block blk_-1608999687919862906 to /10.251.31.5:50010\\n'],\n",
       "       ['081109 203527 149 INFO dfs.DataNode$DataXceiver: 10.250.10.6:50010 Served block blk_-1608999687919862906 to /10.250.18.114\\n'],\n",
       "       ['081109 203527 150 INFO dfs.DataNode$DataXceiver: 10.250.10.6:50010 Served block blk_-1608999687919862906 to /10.251.203.149\\n'],\n",
       "       ['081109 203527 150 INFO dfs.DataNode$DataXceiver: 10.250.11.100:50010 Served block blk_-3544583377289625738 to /10.251.31.5\\n'],\n",
       "       ['081109 203527 150 INFO dfs.DataNode$DataXceiver: 10.250.14.224:50010 Served block blk_-1608999687919862906 to /10.251.31.5\\n'],\n",
       "       ['081109 203527 150 INFO dfs.DataNode$DataXceiver: 10.251.39.179:50010 Served block blk_-3544583377289625738 to /10.251.203.129\\n'],\n",
       "       ['081109 203527 151 INFO dfs.DataNode$DataXceiver: 10.250.10.6:50010 Served block blk_-1608999687919862906 to /10.251.125.193\\n'],\n",
       "       ['081109 203527 151 INFO dfs.DataNode$DataXceiver: 10.250.11.100:50010 Served block blk_-3544583377289625738 to /10.250.19.16\\n'],\n",
       "       ['081109 203527 151 INFO dfs.DataNode$DataXceiver: 10.250.14.224:50010 Served block blk_-1608999687919862906 to /10.251.30.134\\n'],\n",
       "       ['081109 203527 151 INFO dfs.DataNode$DataXceiver: 10.251.39.179:50010 Served block blk_-3544583377289625738 to /10.251.127.243\\n'],\n",
       "       ['081109 203527 152 INFO dfs.DataNode$DataXceiver: 10.250.10.6:50010 Served block blk_-1608999687919862906 to /10.251.65.203\\n'],\n",
       "       ['081109 203527 152 INFO dfs.DataNode$DataXceiver: 10.250.11.100:50010 Served block blk_-3544583377289625738 to /10.251.125.193\\n'],\n",
       "       ['081109 203527 152 INFO dfs.DataNode$DataXceiver: 10.250.14.224:50010 Served block blk_-1608999687919862906 to /10.251.127.191\\n'],\n",
       "       ['081109 203527 152 INFO dfs.DataNode$DataXceiver: 10.251.111.209:50010 Served block blk_-1608999687919862906 to /10.251.111.209\\n'],\n",
       "       ['081109 203527 152 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.251.127.191\\n'],\n",
       "       ['081109 203527 152 INFO dfs.DataNode$DataXceiver: 10.251.39.179:50010 Served block blk_-3544583377289625738 to /10.251.203.149\\n'],\n",
       "       ['081109 203527 153 INFO dfs.DataNode$DataXceiver: 10.250.11.100:50010 Served block blk_-3544583377289625738 to /10.251.30.179\\n'],\n",
       "       ['081109 203527 153 INFO dfs.DataNode$DataXceiver: 10.251.197.226:50010 Served block blk_-3544583377289625738 to /10.250.17.225\\n'],\n",
       "       ['081109 203527 153 INFO dfs.DataNode$DataXceiver: 10.251.215.16:50010 Served block blk_-1608999687919862906 to /10.251.215.16\\n'],\n",
       "       ['081109 203527 153 INFO dfs.DataNode$DataXceiver: 10.251.39.179:50010 Served block blk_-3544583377289625738 to /10.251.30.134\\n'],\n",
       "       ['081109 203527 154 INFO dfs.DataNode$DataXceiver: 10.250.11.100:50010 Served block blk_-3544583377289625738 to /10.251.74.192\\n']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[50:100].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-senate",
   "metadata": {},
   "source": [
    "## Choix du Parseur Drain pour Structurer les Logs Non Structurés\n",
    "\n",
    "Les données que nous utilisons sont des fichiers de logs non structurés. \n",
    "En consultant :\n",
    "- **ICSE'19** : [Tools and Benchmarks for Automated Log Parsing](https://arxiv.org/pdf/1811.03509.pdf), par Jieming Zhu, Shilin He, Jinyang Liu, et al.\n",
    "\n",
    "On constate que le parseur Drain a été identifié comme l'un des plus précis pour cette tâche. \n",
    "\n",
    "Drain est une méthode de parsing de logs qui utilise une structure d’arbre pour créer des modèles fixes (ou \"templates\") de messages. Cette méthode de parsing est intégrée dans l’outil Logparser, qui propose plusieurs méthodes de parsing de logs automatisées.\n",
    "\n",
    "L’outil Logparser, qui inclut l’implémentation de Drain, est disponible dans le [Logparser toolkit](https://github.com/logpai/logparser). Il est cependant à noter que Drain est initialement implémenté en Python 2.7, et l’exécution doit se faire dans cet environnement. Dans notre cas, le script `project_parser.py` utilise Drain pour parser les fichiers `HDFS_2k.log` et `HDFS_2k_train.log`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-memorabilia",
   "metadata": {},
   "source": [
    "## Séparation des Données en Entraînement et Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-treasurer",
   "metadata": {},
   "source": [
    "Pour évaluer la performance de notre modèle de détection d'anomalies, nous allons diviser les données en deux ensembles : un ensemble d'entraînement (80 % des données) et un ensemble de test (20 % des données). En gardant l'ordre des logs, nous utilisons les dernières données pour le test, simulant ainsi des \"nouvelles anomalies\" à découvrir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sized-sentence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indice de séparation entre entraînement et test : 8940503\n"
     ]
    }
   ],
   "source": [
    "# Calcul de l'indice où se termine l'ensemble d'entraînement (80 % des données)\n",
    "train_idx = int(totaln * 0.8)\n",
    "print(\"Indice de séparation entre entraînement et test :\", train_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0432ce",
   "metadata": {},
   "source": [
    "### Création du Fichier d’Entraînement HDFS_2k_train.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "subsequent-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des 80 % premières lignes pour créer un fichier de logs d'entraînement.\n",
    "# Ces données seront utilisées pour générer les modèles de templates dans l’ensemble d’entraînement.\n",
    "with open('DataSet_HDFS/HDFS.log', \"r\") as file:\n",
    "    train_data = list(islice(file, train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stupid-scene",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'DataSet_HDFS/HDFS_train.log'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# write to the training file `HDFS_train.log`\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataSet_HDFS/HDFS_train.log\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[0;32m      4\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(i)\n",
      "File \u001b[1;32mc:\\Users\\LamiaLaraqui\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'DataSet_HDFS/HDFS_train.log'"
     ]
    }
   ],
   "source": [
    "# write to the training file `HDFS_train.log`\n",
    "with open('DataSet_HDFS/HDFS_train.log', 'x') as file:\n",
    "    for i in train_data:\n",
    "        file.write(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee082d62",
   "metadata": {},
   "source": [
    "### /!\\ : AVANT DE PASSER A LA SUITE, IL FAUT MAINTENANT EXECUTER LE CODE `project_parser.py` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-divorce",
   "metadata": {},
   "source": [
    "## Création des Fichiers Structurés de Sortie (après parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-pepper",
   "metadata": {},
   "source": [
    "*Génération des Fichiers de Logs Structurés*\n",
    "\n",
    "Une fois le parsing terminé avec `Drain`, nous obtenons deux fichiers :\n",
    "- `HDFS_2k_train.log_structured.csv` : les logs structurés de l’ensemble d'entraînement.\n",
    "- `HDFS_2k.log_structured.csv` : les logs structurés de tout le fichier `HDFS.log`.\n",
    "\n",
    "Nous allons maintenant extraire les logs de test de `HDFS_2k.log_structured.csv` pour créer `HDFS_2k_test.log_structured.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "occasional-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parsed = pd.read_csv('DataSet_HDFS/HDFS.log_structured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sized-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parsed = all_parsed.iloc[train_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "least-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parsed.to_csv('DataSet_HDFS/HDFS_test.log_structured.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
